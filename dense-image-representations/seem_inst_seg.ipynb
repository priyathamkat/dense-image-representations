{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f728f0cd-09fe-4b9f-9e3a-bf54e0494aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/nehamk/miniconda3/envs/work/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deformable Transformer Encoder is not available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/cmlscratch/nehamk/Segment-Everything-Everywhere-All-At-Once')\n",
    "\n",
    "from seem_module.modeling.BaseModel import BaseModel\n",
    "from seem_module.modeling import build_model\n",
    "from seem_module.utils.arguments import load_opt_from_config_files\n",
    "from seem_module.utils.constants import COCO_PANOPTIC_CLASSES\n",
    "from seem_module.utils.distributed import init_distributed\n",
    "from seem_module.utils.visualizer import Visualizer\n",
    "\n",
    "\n",
    "import os\n",
    "import torch \n",
    "from detectron2.utils.colormap import random_color\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BitMasks\n",
    "from PIL import Image\n",
    "from torchvision import transforms \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd7599f9-2c20-4379-942a-cefe61d95133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n"
     ]
    }
   ],
   "source": [
    "conf_files = 'configs/seem/focall_unicl_lang_demo.yaml'\n",
    "opt = load_opt_from_config_files([conf_files])\n",
    "opt = init_distributed(opt)\n",
    "\n",
    "# META DATA\n",
    "cur_model = 'None'\n",
    "if 'focalt' in conf_files:\n",
    "    pretrained_pth = os.path.join(\"seem_focalt_v0.pt\")\n",
    "    if not os.path.exists(pretrained_pth):\n",
    "        os.system(\"wget {}\".format(\"https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focalt_v0.pt\"))\n",
    "    cur_model = 'Focal-T'\n",
    "elif 'focal' in conf_files:\n",
    "    pretrained_pth = os.path.join(\"seem_focall_v0.pt\")\n",
    "    if not os.path.exists(pretrained_pth):\n",
    "        os.system(\"wget {}\".format(\"https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focall_v0.pt\"))\n",
    "    cur_model = 'Focal-L'\n",
    "\n",
    "'''\n",
    "build model\n",
    "'''\n",
    "model = BaseModel(opt, build_model(opt)).from_pretrained(pretrained_pth).eval().cuda()\n",
    "with torch.no_grad():\n",
    "    # model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(COCO_PANOPTIC_CLASSES + [\"background\"], is_eval=True)\n",
    "\n",
    "    thing_colors = [random_color(rgb=True, maximum=255).astype(np.int32).tolist() for _ in range(len(COCO_PANOPTIC_CLASSES))]\n",
    "    thing_dataset_id_to_contiguous_id = {x:x for x in range(len(COCO_PANOPTIC_CLASSES))}\n",
    "    \n",
    "    MetadataCatalog.get(\"demo\").set(\n",
    "        thing_colors=thing_colors,\n",
    "        thing_classes=COCO_PANOPTIC_CLASSES,\n",
    "        thing_dataset_id_to_contiguous_id=thing_dataset_id_to_contiguous_id,\n",
    "    )\n",
    "    model.model.sem_seg_head.predictor.lang_encoder.get_text_embeddings(COCO_PANOPTIC_CLASSES + [\"background\"], is_eval=True)\n",
    "    metadata = MetadataCatalog.get('demo')\n",
    "    model.model.metadata = metadata\n",
    "    model.model.sem_seg_head.num_classes = len(COCO_PANOPTIC_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c2f23f-945c-4fff-bbc6-b3d773d1703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# t = []\n",
    "# t.append(transforms.Resize((512, 512), interpolation=Image.BICUBIC))\n",
    "# transform = transforms.Compose(t)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize((512, 512), interpolation=Image.BICUBIC),\n",
    "                transforms.PILToTensor()\n",
    "            ])\n",
    "\n",
    "# image_ori = Image.open('person_with_coffee.jpeg').convert(\"RGB\")\n",
    "image_ori = Image.open('/fs/cml-datasets/coco/images/train2017/000000000009.jpg').convert(\"RGB\")\n",
    "\n",
    "width = image_ori.size[0]\n",
    "height = image_ori.size[1]\n",
    "images = transform(image_ori)\n",
    "# image = np.asarray(image)\n",
    "print(image.shape)\n",
    "image_ori = np.asarray(image_ori)\n",
    "# images = torch.from_numpy(image.copy()).permute(2,0,1).cuda()\n",
    "\n",
    "batch_inputs = [{'image': images, 'height': height, 'width': width}]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.forward(batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e256763-9a66-4619-9b13-48e129710981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfb9fe1a-ff25-4a26-a3fd-69dd8e6d2c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visual = Visualizer(image_ori, metadata=metadata)\n",
    "\n",
    "inst_seg = outputs[-1]['instances']\n",
    "inst_seg.pred_masks = inst_seg.pred_masks.cpu()\n",
    "inst_seg.pred_boxes = BitMasks(inst_seg.pred_masks > 0).get_bounding_boxes()\n",
    "demo = visual.draw_instance_predictions(inst_seg) # rgb Image\n",
    "\n",
    "demo.save('inst.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d865cd70-38c9-4fa5-83f1-024e29aa9d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instances(num_instances=100, image_height=404, image_width=715, fields=[pred_masks: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]]), pred_boxes: Boxes(tensor([[  0.,   0.,   0.,   0.],\n",
       "        [ 11., 142., 520., 404.],\n",
       "        [ 11., 142., 520., 404.],\n",
       "        [  0., 235.,  30., 272.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [474., 203., 479., 217.],\n",
       "        [474., 203., 479., 217.],\n",
       "        [474., 203., 479., 217.],\n",
       "        [  0., 247.,  64., 404.],\n",
       "        [165., 210., 322., 255.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0., 361.,  63., 404.],\n",
       "        [  0., 211.,  32., 275.],\n",
       "        [527., 209., 539., 217.],\n",
       "        [527., 209., 539., 217.],\n",
       "        [516., 194., 541., 219.],\n",
       "        [516., 194., 541., 219.],\n",
       "        [516., 194., 541., 219.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0., 361.,  64., 404.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [516., 194., 541., 219.],\n",
       "        [ 10., 111., 524., 404.],\n",
       "        [474., 209., 479., 217.],\n",
       "        [474., 209., 479., 217.],\n",
       "        [ 10., 110., 524., 404.],\n",
       "        [ 10., 110., 524., 404.],\n",
       "        [518., 196., 540., 218.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [526., 195., 540., 218.],\n",
       "        [  0., 354., 100., 404.],\n",
       "        [513., 194., 541., 218.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0., 361.,  62., 404.],\n",
       "        [  0., 361.,  62., 404.],\n",
       "        [  0., 227., 715., 359.],\n",
       "        [  0., 235.,  30., 271.],\n",
       "        [  0., 299., 715., 404.],\n",
       "        [231., 192., 319., 201.],\n",
       "        [231., 192., 319., 201.],\n",
       "        [515., 193., 541., 219.],\n",
       "        [513., 191., 630., 220.],\n",
       "        [513., 191., 630., 220.],\n",
       "        [513., 191., 630., 220.],\n",
       "        [513., 196., 541., 219.],\n",
       "        [  0., 358.,  66., 404.],\n",
       "        [525., 196., 540., 218.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [525., 195., 540., 218.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [ 30., 112., 519., 331.],\n",
       "        [  0., 358.,  70., 404.],\n",
       "        [  0., 358.,  70., 404.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [514., 193., 630., 220.],\n",
       "        [514., 193., 630., 220.],\n",
       "        [514., 193., 630., 220.],\n",
       "        [514., 193., 630., 220.],\n",
       "        [  0., 210.,  31., 275.],\n",
       "        [474., 202., 540., 218.],\n",
       "        [474., 202., 540., 218.],\n",
       "        [  0., 361.,  63., 404.],\n",
       "        [221., 167., 245., 185.],\n",
       "        [  0., 352.,  72., 404.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.],\n",
       "        [225., 176., 321., 212.],\n",
       "        [  0., 299., 715., 404.],\n",
       "        [150.,  57., 342., 385.],\n",
       "        [ 11., 110., 523., 404.],\n",
       "        [516., 202., 631., 221.],\n",
       "        [516., 202., 631., 221.],\n",
       "        [221., 168., 244., 184.],\n",
       "        [  0., 236.,  21., 265.],\n",
       "        [525., 196., 540., 218.],\n",
       "        [  0., 237.,  62., 404.],\n",
       "        [ 12., 110., 522., 404.],\n",
       "        [513., 193., 541., 219.],\n",
       "        [513., 193., 541., 219.],\n",
       "        [513., 193., 541., 219.],\n",
       "        [513., 193., 541., 219.],\n",
       "        [  0.,   0., 715., 255.],\n",
       "        [474., 209., 480., 217.],\n",
       "        [474., 209., 480., 217.],\n",
       "        [229., 177., 321., 212.],\n",
       "        [513., 193., 541., 219.],\n",
       "        [474., 208., 481., 217.],\n",
       "        [474., 208., 481., 217.],\n",
       "        [221., 166., 246., 185.]])), scores: tensor([0.0000, 0.0103, 0.0043, 0.0998, 0.0000, 0.0000, 0.0072, 0.0098, 0.0067,\n",
       "        0.0191, 0.0052, 0.0000, 0.0217, 0.0116, 0.0064, 0.0229, 0.1048, 0.0046,\n",
       "        0.0070, 0.0000, 0.0080, 0.0000, 0.0165, 0.0174, 0.0103, 0.0042, 0.0170,\n",
       "        0.0452, 0.0221, 0.0000, 0.0000, 0.0638, 0.0142, 0.0250, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0055, 0.1305, 0.9118, 0.0060, 0.9563,\n",
       "        0.0092, 0.0065, 0.0130, 0.0102, 0.0054, 0.0052, 0.0233, 0.0190, 0.0162,\n",
       "        0.0000, 0.0217, 0.0000, 0.0000, 0.0000, 0.0000, 0.0075, 0.0049, 0.0165,\n",
       "        0.0000, 0.0123, 0.0057, 0.0055, 0.0055, 0.0109, 0.0056, 0.0389, 0.0166,\n",
       "        0.8279, 0.0151, 0.0000, 0.0000, 0.0000, 0.0078, 0.5646, 0.9550, 0.0212,\n",
       "        0.0040, 0.0046, 0.0049, 0.0148, 0.0246, 0.0806, 0.9351, 0.0664, 0.0043,\n",
       "        0.0070, 0.0043, 0.9786, 0.0151, 0.0225, 0.9678, 0.0164, 0.0145, 0.0216,\n",
       "        0.0039], device='cuda:0'), pred_classes: tensor([123,  13, 125, 126,   0,   2,   0,   2,  14, 126,  26,   2, 126, 126,\n",
       "          0,   2,   2,  99, 113,   0, 126,   2,   2, 125,   0,  14, 116, 125,\n",
       "          2,   2,  13,   2, 126,   2, 100, 101, 109, 119, 123, 126, 123, 126,\n",
       "        125, 126, 123,  73, 127,   2,   2,  91, 129,   2, 126,   2,   2,   2,\n",
       "         92, 119, 129, 131,  13, 123, 126,   0,   2,  91,  99, 129, 126,   0,\n",
       "          2, 126,  41, 126,  14,  26,  67,  73, 122,   0, 125,   2,  99,  41,\n",
       "        126,   2, 126,  13,   2,  99, 113, 129, 116,   0,   2,  73,   2,   0,\n",
       "          2,  41], device='cuda:0')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8225407-23ce-46d6-a4e5-86dc71e2ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XdecoderHead(\n",
       "  (pixel_decoder): TransformerEncoderPixelDecoder(\n",
       "    (adapter_1): Conv2d(\n",
       "      192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (layer_1): Conv2d(\n",
       "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (adapter_2): Conv2d(\n",
       "      384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (layer_2): Conv2d(\n",
       "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (adapter_3): Conv2d(\n",
       "      768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (layer_3): Conv2d(\n",
       "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "    (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (input_proj): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (transformer): TransformerEncoderOnly(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "        num_pos_feats: 256\n",
       "        temperature: 10000\n",
       "        normalize: True\n",
       "        scale: 6.283185307179586\n",
       "    (layer_4): Conv2d(\n",
       "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (predictor): SEEMDecoder(\n",
       "    (pe_layer): Positional encoding PositionEmbeddingSine\n",
       "        num_pos_feats: 256\n",
       "        temperature: 10000\n",
       "        normalize: True\n",
       "        scale: 6.283185307179586\n",
       "    (transformer_self_attention_layers): ModuleList(\n",
       "      (0-8): 9 x SelfAttentionLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (transformer_cross_attention_layers): ModuleList(\n",
       "      (0-8): 9 x CrossAttentionLayer(\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (transformer_ffn_layers): ModuleList(\n",
       "      (0-8): 9 x FFNLayer(\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (query_feat): Embedding(101, 512)\n",
       "    (query_embed): Embedding(101, 512)\n",
       "    (pn_indicator): Embedding(2, 512)\n",
       "    (level_embed): Embedding(3, 512)\n",
       "    (input_proj): ModuleList(\n",
       "      (0-2): 3 x Sequential()\n",
       "    )\n",
       "    (lang_encoder): LanguageEncoder(\n",
       "      (lang_encoder): Transformer(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (resblocks): ModuleList(\n",
       "          (0-11): 12 x ResidualAttentionBlock(\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (ln_1): LayerNorm()\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (ln_2): LayerNorm()\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (ln_final): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (mask_embed): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_sptial_embed): ParameterList(\n",
       "        (0): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
       "        (1): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
       "        (2): Parameter containing: [torch.float32 of size 512x512 (cuda:0)]\n",
       "    )\n",
       "    (spatial_embed): Embedding(32, 512)\n",
       "    (spatial_featured): Embedding(32, 512)\n",
       "    (attention_data): AttentionDataStruct()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.sem_seg_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dfe489-3a27-4dda-85db-995168d31e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
